# autoqa_launcher.py
#!/usr/bin/env python3
"""
MRI AutoQA Launcher - One-command automated batch quality analysis
ä¸€é”®å¯åŠ¨è‡ªåŠ¨åŒ–æ‰¹é‡è´¨é‡åˆ†æï¼Œè‡ªåŠ¨è¾“å‡ºå®Œæ•´æŠ¥å‘Š
ä¿®å¤ç‰ˆï¼šæ·»åŠ æ‰«æå‚æ•°æ¸…å•å¯¼å‡ºåŠŸèƒ½
"""

import argparse
import sys
import time
from pathlib import Path
from datetime import datetime
import shutil
import json
import pandas as pd
from typing import Dict, Any, Optional, List
import numpy as np

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent

class AutoQALauncher:
    """
    è‡ªåŠ¨åŒ–MRIè´¨é‡åˆ†æå¯åŠ¨å™¨
    å•ä¸ªå‘½ä»¤å®Œæˆï¼šæ•°æ®éªŒè¯ â†’ æ‰¹é‡å¤„ç† â†’ æŠ¥å‘Šç”Ÿæˆ â†’ å¯è§†åŒ–
    é‡æ–°è®¾è®¡æŠ¥å‘Šè¾“å‡ºç³»ç»Ÿï¼Œå¸¦æ—¶é—´æˆ³çš„åˆ†å±‚æŠ¥å‘Š
    """
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.start_time = None
        self.end_time = None
        self.results_dir = None
        self.report_timestamp = None
        self.report_id = None
        self.scan_count = 0
        
        # é»˜è®¤è·¯å¾„é…ç½®
        self.paths = {
            'input_data': PROJECT_ROOT / "converted_data",
            'output_results': PROJECT_ROOT / "autoqa_results",
            'batch_report': PROJECT_ROOT / "batch_reports",
            'visualizations': PROJECT_ROOT / "visualizations",
            'logs': PROJECT_ROOT / "logs"
        }
        
        # æ¨¡å—å¯ç”¨æ€§æ ‡å¿—
        self.modules_available = {
            'skimage': False,
            'visualization': False,
            'batch_visualization': False
        }
        
        self.log("=" * 70)
        self.log("MRI AutoQA Launcher - Automated Batch Quality Analysis")
        self.log("é‡æ–°è®¾è®¡æŠ¥å‘Šè¾“å‡ºç³»ç»Ÿ v2.0")
        self.log("=" * 70)
        
    def run_full_pipeline(self, 
                         input_dir: str = None,
                         output_dir: str = None,
                         skip_visualization: bool = False,
                         force_clean: bool = False) -> bool:
        """
        è¿è¡Œå®Œæ•´åˆ†ææµæ°´çº¿
        
        Args:
            input_dir: è¾“å…¥æ•°æ®ç›®å½•ï¼ˆå¦‚æœªæŒ‡å®šä½¿ç”¨é»˜è®¤ï¼‰
            output_dir: è¾“å‡ºç»“æœç›®å½•ï¼ˆå¦‚æœªæŒ‡å®šä½¿ç”¨é»˜è®¤ï¼‰
            skip_visualization: æ˜¯å¦è·³è¿‡å¯è§†åŒ–ç”Ÿæˆ
            force_clean: æ˜¯å¦æ¸…ç†æ—§ç»“æœ
            
        Returns:
            æˆåŠŸçŠ¶æ€
        """
        self.start_time = datetime.now()
        self.report_timestamp = self.start_time.strftime("%Y%m%d_%H%M%S")
        self.report_id = f"AUTOQA-{self.report_timestamp}"
        
        try:
            # 1. éªŒè¯å’Œå‡†å¤‡ç¯å¢ƒ
            self._validate_environment()
            
            # 2. è®¾ç½®è¾“å…¥è¾“å‡ºç›®å½•
            self._setup_directories(input_dir, output_dir, force_clean)
            
            # 3. æ£€æŸ¥è¾“å…¥æ•°æ®
            self.scan_count = self._check_input_data()
            if self.scan_count == 0:
                self.log("âŒ æœªæ‰¾åˆ°ä»»ä½•æ‰«ææ•°æ®ï¼Œè¯·æ£€æŸ¥è¾“å…¥ç›®å½•")
                return False
            
            self.log(f"âœ… æ‰¾åˆ° {self.scan_count} ä¸ªå¾…åˆ†ææ‰«æ")
            
            # 4. è¿è¡Œæ‰¹é‡åˆ†æ
            if not self._run_batch_analysis():
                return False
            
            # 5. ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„åˆ†å±‚æŠ¥å‘Š
            if not self._generate_timestamped_reports():
                return False
            
            # 6. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šï¼ˆå¯é€‰ï¼‰
            if not skip_visualization:
                if not self._generate_visualizations():
                    self.log("âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥ï¼Œç»§ç»­å…¶ä»–æ­¥éª¤")
            
            # 7. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files()
            
            # 8. åˆ›å»ºæœ€æ–°æŠ¥å‘Šç¬¦å·é“¾æ¥
            self._create_latest_symlink()
            
            self.end_time = datetime.now()
            duration = (self.end_time - self.start_time).total_seconds()
            
            self._print_success_summary(duration, self.scan_count)
            return True
            
        except Exception as e:
            self.log(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _validate_environment(self):
        """éªŒè¯Pythonç¯å¢ƒå’Œä¾èµ–"""
        self.log("ğŸ” éªŒè¯ç¯å¢ƒ...")
        
        # æ£€æŸ¥Pythonç‰ˆæœ¬
        python_version = sys.version_info
        if python_version.major < 3 or (python_version.major == 3 and python_version.minor < 8):
            raise EnvironmentError(f"éœ€è¦Python 3.8+ï¼Œå½“å‰ç‰ˆæœ¬: {sys.version}")
        
        self.log(f"   Pythonç‰ˆæœ¬: {sys.version}")
        
        # æ£€æŸ¥å¿…è¦æ¨¡å—ï¼ˆæ ¸å¿ƒåŠŸèƒ½å¿…éœ€ï¼‰
        required_modules = [
            'numpy', 'nibabel', 'matplotlib', 'pandas', 
            'scipy', 'psutil'
        ]
        
        # æ£€æŸ¥å¯é€‰æ¨¡å—
        optional_modules = [
            ('skimage', 'scikit-image', 'é«˜çº§å›¾åƒå¤„ç†'),
            ('sklearn', 'scikit-learn', 'æœºå™¨å­¦ä¹ åˆ†æ'),
            ('seaborn', 'seaborn', 'å¯è§†åŒ–ç¾åŒ–')
        ]
        
        missing_required = []
        missing_optional = []
        
        # æ£€æŸ¥æ ¸å¿ƒå¿…éœ€æ¨¡å—
        for module in required_modules:
            try:
                __import__(module)
                self.log(f"   âœ“ {module} (å¿…éœ€)")
            except ImportError:
                missing_required.append(module)
                self.log(f"   âœ— {module} (å¿…éœ€ï¼Œç¼ºå¤±)")
        
        # æ£€æŸ¥å¯é€‰æ¨¡å—
        for module_name, pip_name, description in optional_modules:
            try:
                __import__(module_name)
                self.log(f"   âœ“ {module_name} (å¯é€‰)")
                # è®°å½•skimageæ˜¯å¦å¯ç”¨
                if module_name == 'skimage':
                    self.modules_available['skimage'] = True
            except ImportError:
                missing_optional.append((module_name, pip_name, description))
                self.log(f"   âš  {module_name} (å¯é€‰ï¼Œç¼ºå¤±)")
        
        # å¦‚æœæœ‰ç¼ºå¤±çš„å¿…éœ€æ¨¡å—ï¼ŒæŠ¥é”™
        if missing_required:
            self.log(f"\nâŒ ç¼ºå¤±æ ¸å¿ƒå¿…éœ€æ¨¡å—: {', '.join(missing_required)}")
            self.log("è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…:")
            self.log(f"   pip install {' '.join(missing_required)}")
            raise ImportError(f"ç¼ºå¤±æ ¸å¿ƒæ¨¡å—: {missing_required}")
        
        # å¦‚æœæœ‰ç¼ºå¤±çš„å¯é€‰æ¨¡å—ï¼Œè­¦å‘Šä½†ä¸ä¸­æ–­
        if missing_optional:
            self.log(f"\nâš ï¸  ç¼ºå¤±å¯é€‰æ¨¡å—:")
            for module_name, pip_name, description in missing_optional:
                self.log(f"   - {module_name}: {description}")
            
            self.log("\n  è¿™äº›æ¨¡å—ç”¨äºå¢å¼ºåŠŸèƒ½ï¼Œä¸å½±å“æ ¸å¿ƒåˆ†æ:")
            self.log("  å¦‚éœ€å®‰è£…: pip install " + " ".join([name for _, name, _ in missing_optional]))
        
        # æ£€æŸ¥é¡¹ç›®æ¨¡å—
        required_project_modules = [
            'single_imagine.py',
            'batch_processor.py', 
            'run_analysis.py',
            'config.py'
        ]
        
        # å¯è§†åŒ–æ¨¡å—æ˜¯å¯é€‰çš„
        optional_project_modules = [
            ('visualization.py', 'å¯è§†åŒ–ç”Ÿæˆå™¨'),
            ('batch_visualization.py', 'æ‰¹é‡å¯è§†åŒ–'),
            ('visualization_config.py', 'å¯è§†åŒ–é…ç½®')
        ]
        
        for module in required_project_modules:
            module_path = PROJECT_ROOT / module
            if module_path.exists():
                self.log(f"   âœ“ {module} (é¡¹ç›®æ–‡ä»¶)")
            else:
                self.log(f"   âœ— {module} (é¡¹ç›®æ–‡ä»¶ï¼Œç¼ºå¤±)")
                raise FileNotFoundError(f"é¡¹ç›®æ–‡ä»¶ç¼ºå¤±: {module}")
        
        # æ£€æŸ¥å¯è§†åŒ–æ¨¡å—
        viz_modules_exist = []
        for module, description in optional_project_modules:
            module_path = PROJECT_ROOT / module
            if module_path.exists():
                self.log(f"   âœ“ {module} ({description})")
                viz_modules_exist.append(True)
                
                # è®°å½•å¯è§†åŒ–æ¨¡å—å¯ç”¨æ€§ - ä½¿ç”¨ç²¾ç¡®åŒ¹é…
                if module == 'visualization.py':
                    self.modules_available['visualization'] = True
                elif module == 'batch_visualization.py':
                    self.modules_available['batch_visualization'] = True
            else:
                self.log(f"   âš  {module} ({description}ï¼Œç¼ºå¤±)")
                viz_modules_exist.append(False)
        
        # å¦‚æœæ‰€æœ‰å¯è§†åŒ–æ¨¡å—éƒ½å­˜åœ¨ï¼Œæ ‡è®°ä¸ºå¯ç”¨
        if all(viz_modules_exist):
            self.log("   âœ“ æ‰€æœ‰å¯è§†åŒ–æ¨¡å—å¯ç”¨")
        else:
            self.log("   âš  éƒ¨åˆ†å¯è§†åŒ–æ¨¡å—ç¼ºå¤±ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™")
        
        self.log("âœ… ç¯å¢ƒéªŒè¯é€šè¿‡")
    
    def _setup_directories(self, input_dir: str, output_dir: str, force_clean: bool):
        """è®¾ç½®è¾“å…¥è¾“å‡ºç›®å½•"""
        self.log("ğŸ“ è®¾ç½®ç›®å½•...")
        
        # 1. å¤„ç†è¾“å…¥ç›®å½•
        if input_dir:
            input_path = Path(input_dir)
            if not input_path.exists():
                raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_path}")
            self.paths['input_data'] = input_path
        
        input_data_path = self.paths['input_data']
        if not input_data_path.exists():
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_data_path}")
        
        self.log(f"   è¾“å…¥ç›®å½•: {input_data_path.absolute()}")
        
        # 2. å¤„ç†è¾“å‡ºç›®å½•
        if output_dir:
            output_path = Path(output_dir)
            self.paths['output_results'] = output_path
        
        # ç¡®ä¿results_dirè¢«è®¾ç½®
        self.results_dir = self.paths['output_results']
        
        # 3. åˆ›å»ºæ‰€æœ‰è¾“å‡ºç›®å½•
        self.log(f"   åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„...")
        for key, path in self.paths.items():
            if key != 'input_data':
                try:
                    path.mkdir(parents=True, exist_ok=True)
                    if self.verbose:
                        self.log(f"     - {key}: {path.absolute()}")
                except Exception as e:
                    self.log(f"     âš ï¸ åˆ›å»ºç›®å½•å¤±è´¥ {key}: {e}")
        
        # 4. å¼ºåˆ¶æ¸…ç†ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if force_clean and self.results_dir.exists():
            self.log(f"âš ï¸ å¼ºåˆ¶æ¸…ç†æ¨¡å¼: åˆ é™¤æ—§ç»“æœç›®å½•")
            try:
                # å…ˆåˆ é™¤ç›®å½•å†…å®¹
                for item in self.results_dir.iterdir():
                    if item.is_file():
                        item.unlink()
                    elif item.is_dir():
                        shutil.rmtree(item)
                self.log(f"   æ¸…ç†å®Œæˆ: {self.results_dir}")
            except Exception as e:
                self.log(f"   æ¸…ç†å¤±è´¥: {e}")
        
        # 5. æœ€ç»ˆç¡®è®¤
        self.log(f"âœ… ç›®å½•è®¾ç½®å®Œæˆ")
        self.log(f"   è¾“å…¥ç›®å½•: {self.paths['input_data'].absolute()}")
        self.log(f"   è¾“å‡ºç›®å½•: {self.results_dir.absolute()}")
        
        # ç¡®ä¿results_dirå­˜åœ¨
        if not self.results_dir.exists():
            self.results_dir.mkdir(parents=True, exist_ok=True)
            self.log(f"   åˆ›å»ºè¾“å‡ºç›®å½•: {self.results_dir.absolute()}")
    
    def _check_input_data(self) -> int:
        """æ£€æŸ¥è¾“å…¥æ•°æ®ï¼Œè¿”å›æ‰«ææ•°é‡"""
        input_dir = self.paths['input_data']
    
        self.log("ğŸ” æ£€æŸ¥è¾“å…¥æ•°æ®...")
        self.log(f"   è¾“å…¥ç›®å½•: {input_dir.absolute()}")
    
        # æŸ¥æ‰¾æ‰€æœ‰NIfTIæ–‡ä»¶
        nifti_files = list(input_dir.rglob("*.nii.gz")) + list(input_dir.rglob("*.nii"))
    
        if not nifti_files:
            self.log("   æœªæ‰¾åˆ°NIfTIæ–‡ä»¶")
            return 0
    
        scan_count = 0
        patients = {}
    
        for nifti_path in nifti_files:
            # æå–æ‚£è€…IDå’Œæ‰«æåç§°
            try:
                rel_path = nifti_path.relative_to(input_dir)
                parts = rel_path.parts
            
                if len(parts) >= 2:
                    patient_id = parts[0]
                    scan_name = parts[1] if len(parts) > 1 else nifti_path.stem
                
                    if patient_id not in patients:
                        patients[patient_id] = set()
                    patients[patient_id].add(scan_name)
                
                    scan_count += 1
            except Exception as e:
                # å¦‚æœæ— æ³•è§£æè·¯å¾„ï¼Œåªè®¡æ•°
                self.log(f"   è­¦å‘Š: æ— æ³•è§£æè·¯å¾„ {nifti_path}: {e}")
                scan_count += 1
    
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        self.log(f"   æ‰¾åˆ° {scan_count} ä¸ªNIfTIæ–‡ä»¶")
        if patients:
            self.log(f"   æ¶‰åŠ {len(patients)} ä¸ªæ‚£è€…")
            for patient_id, scans in list(patients.items())[:5]:
                self.log(f"     - {patient_id}: {len(scans)} ä¸ªæ‰«æ")
            if len(patients) > 5:
                self.log(f"     ... å’Œ {len(patients) - 5} ä¸ªæ›´å¤šæ‚£è€…")
        else:
            self.log("   è­¦å‘Š: æ— æ³•æŒ‰æ‚£è€…ç»„ç»‡æ–‡ä»¶")
    
        return scan_count
    
    def _run_batch_analysis(self) -> bool:
        """è¿è¡Œæ‰¹é‡åˆ†æ"""
        self.log("\nğŸš€ å¼€å§‹æ‰¹é‡è´¨é‡åˆ†æ...")
    
        try:
            # å¯¼å…¥æ‰¹é‡å¤„ç†å™¨
            sys.path.append(str(PROJECT_ROOT))
            from batch_processor import run_batch_processing
        
            # è¿è¡Œæ‰¹é‡å¤„ç†
            stats = run_batch_processing(
                input_dir=str(self.paths['input_data']),
                output_dir=str(self.results_dir),
                verbose=self.verbose
            )
        
            # æ£€æŸ¥ç»“æœ
            if stats.get('successful', 0) == 0 and stats.get('total_scans', 0) > 0:
                self.log("âŒ æ‰¹é‡åˆ†æå¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸå¤„ç†çš„æ‰«æ")
                return False
        
            self.log(f"âœ… æ‰¹é‡åˆ†æå®Œæˆ")
            self.log(f"   æ€»è®¡æ‰«æ: {stats.get('total_scans', 0)}")
            self.log(f"   æˆåŠŸåˆ†æ: {stats.get('successful', 0)}")
            self.log(f"   åˆ†æå¤±è´¥: {stats.get('failed', 0)}")
            self.log(f"   è·³è¿‡æ‰«æ: {stats.get('skipped', 0)}")
            self.log(f"   å¤„ç†æ—¶é—´: {stats.get('duration_seconds', 0):.1f}ç§’")
        
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_file = self.results_dir / "analysis_statistics.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False, default=str)
        
            return True
        
        except Exception as e:
            self.log(f"âŒ æ‰¹é‡åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _generate_timestamped_reports(self) -> bool:
        """
        ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„åˆ†å±‚æŠ¥å‘Šç³»ç»Ÿ
        ä¸»å…¥å£å‡½æ•°ï¼Œè°ƒç”¨å„ä¸ªå­æŠ¥å‘Šç”Ÿæˆå‡½æ•°
        """
        self.log(f"\nğŸ“Š ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„åˆ†å±‚æŠ¥å‘Šç³»ç»Ÿ...")
        self.log(f"   æŠ¥å‘ŠID: {self.report_id}")
        self.log(f"   æ—¶é—´æˆ³: {self.report_timestamp}")
        
        try:
            # åˆ›å»ºä¸»æŠ¥å‘Šç›®å½•
            report_dir = self.results_dir / f"batch_report_{self.report_timestamp}"
            report_dir.mkdir(exist_ok=True)
            
            # åˆ›å»ºå­ç›®å½•ç»“æ„
            subdirs = [
                "00_executive_summary",
                "01_detailed_data", 
                "02_visualizations",
                "03_quality_analysis",
                "04_technical_appendix"
            ]
            
            for subdir in subdirs:
                (report_dir / subdir).mkdir(exist_ok=True)
            
            # 1. æå–æ‰€æœ‰æ•°æ®
            self.log("   1. æå–æ‰«ææ•°æ®...")
            all_results = self._extract_all_scan_data()
            if not all_results:
                self.log("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ç»“æœæ•°æ®")
                return False
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(all_results)
            
            # 2. ç”Ÿæˆå„ç§æŠ¥å‘Š
            self.log("   2. ç”Ÿæˆæ‰§è¡Œæ‘˜è¦...")
            self._generate_executive_summary(df, report_dir)
            
            self.log("   3. ç”Ÿæˆè¯¦ç»†æ•°æ®æ–‡ä»¶...")
            self._generate_detailed_data_files(df, report_dir)
            
            self.log("   4. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
            self._generate_statistical_reports(df, report_dir)
            
            self.log("   5. ç”Ÿæˆè´¨é‡åˆ†ææŠ¥å‘Š...")
            self._generate_quality_analysis(df, report_dir)
            
            self.log("   6. ç”ŸæˆæŠ€æœ¯é™„å½•...")
            self._generate_technical_appendix(df, report_dir)
            
            # 7. åˆ›å»ºæŠ¥å‘Šç´¢å¼•æ–‡ä»¶
            self._create_report_index(report_dir, df)
            
            self.log(f"âœ… åˆ†å±‚æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            self.log(f"   æŠ¥å‘Šç›®å½•: {report_dir.absolute()}")
            
            return True
            
        except Exception as e:
            self.log(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _extract_all_scan_data(self) -> List[Dict]:
        """æå–æ‰€æœ‰æ‰«ææ•°æ®"""
        result_files = list(self.results_dir.rglob("**/qa_report.json"))
        
        if not result_files:
            self.log("âŒ æœªæ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶")
            return []
        
        self.log(f"   æ‰¾åˆ° {len(result_files)} ä¸ªåˆ†æç»“æœ")
        
        all_results = []
        
        for result_file in result_files:
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                
                # æå–æ‰«æä¿¡æ¯
                scan_info = self._extract_scan_info_v2(result_data, result_file)
                if scan_info:
                    all_results.append(scan_info)
                    
            except Exception as e:
                self.log(f"   è­¦å‘Š: æ— æ³•å¤„ç† {result_file}: {e}")
        
        return all_results
    
    def _extract_scan_info_v2(self, result_data: Dict[str, Any], result_file: Path) -> Optional[Dict]:
        """
        æå–æ‰«æä¿¡æ¯ - ä¿®å¤ç‰ˆï¼Œä¸JSONç»“æ„ä¸€è‡´
        """
        try:
            # åŸºç¡€ä¿¡æ¯
            scan_info = {
                # æŠ¥å‘Šæ ‡è¯†
                'scan_id': result_data.get('analysis_info', {}).get('scan_id', ''),
                'analysis_date': result_data.get('analysis_info', {}).get('date', ''),
                'algorithm_version': result_data.get('analysis_info', {}).get('algorithm_version', ''),
                'analysis_status': result_data.get('analysis_status', 'UNKNOWN'),
                
                # ä¸´åºŠä¿¡æ¯
                'patient_id': result_data.get('acquisition', {}).get('patient_id', ''),
                'scan_name': result_data.get('acquisition', {}).get('scan_name', ''),
                'anatomical_region': result_data.get('acquisition', {}).get('anatomical_region', ''),
                'sequence_type': result_data.get('acquisition', {}).get('sequence_type', ''),
                'field_strength': result_data.get('acquisition', {}).get('field_strength', ''),
                'acquisition_mode': result_data.get('acquisition', {}).get('acquisition_mode', ''),
                'acceleration_factor': result_data.get('acquisition', {}).get('acceleration_factor', 1.0),
                'parallel_imaging': result_data.get('acquisition', {}).get('parallel_imaging', False),
                
                # éªŒè¯ä¿¡æ¯
                'validation_status': result_data.get('validation_info', {}).get('status', ''),
            }
            
            # å¦‚æœæ˜¯å¤±è´¥æ‰«æï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
            if scan_info['analysis_status'] != 'COMPLETED':
                scan_info['error_message'] = result_data.get('error', '')
                scan_info['error_type'] = result_data.get('error_type', '')
                return scan_info
            
            # æˆåŠŸæ‰«æï¼Œæå–è¯¦ç»†ç»“æœ
            
            # 1. SNRç›¸å…³
            snr_results = result_data.get('snr_results', {})
            traditional = snr_results.get('traditional', {})
            recommended = snr_results.get('recommended', {})
            rayleigh = snr_results.get('rayleigh_correction', {})
            
            scan_info.update({
                'snr_raw': traditional.get('snr', 0.0),
                'snr_corrected': recommended.get('snr', 0.0),
                'snr_improvement_percent': rayleigh.get('improvement_percent', 0.0),
                'snr_rating': result_data.get('quality_assessment', {}).get('snr_rating', {}).get('level', ''),
                'correction_factor': rayleigh.get('correction_factor', 1.0),
            })
            
            # 2. CNRç›¸å…³
            cnr_analysis = result_data.get('quality_assessment', {}).get('cnr_analysis', {})
            if cnr_analysis and 'best_cnr' in cnr_analysis:
                best_cnr = cnr_analysis['best_cnr']
                scan_info.update({
                    'cnr_value': best_cnr.get('cnr_value', 0.0),
                    'cnr_rating': best_cnr.get('cnr_rating', ''),
                    'cnr_tissue_pair': best_cnr.get('description', ''),
                })
            else:
                scan_info.update({
                    'cnr_value': 0.0,
                    'cnr_rating': 'N/A',
                    'cnr_tissue_pair': '',
                })
            
            # 3. è´¨é‡è¯„åˆ†
            quality_assessment = result_data.get('quality_assessment', {})
            quality_scores = quality_assessment.get('quality_scores', {})
            if quality_scores:
                dimensions = quality_scores.get('dimensions', {})
                scan_info.update({
                    'quality_score_total': quality_scores.get('total_score', 0.0),
                    'quality_snr': dimensions.get('snr_quality', {}).get('score', 0.0),
                    'quality_cnr': dimensions.get('cnr_quality', {}).get('score', 0.0),
                    'quality_noise': dimensions.get('noise_quality', {}).get('score', 0.0),
                    'quality_artifact': dimensions.get('artifact_free', {}).get('score', 0.0),
                })
            
            # 4. ç½®ä¿¡åº¦è¯„ä¼°
            overall_confidence = quality_assessment.get('overall_confidence', {})
            scan_info.update({
                'confidence_score': overall_confidence.get('score', 0.0),
                'algorithm_confidence': overall_confidence.get('level', 'UNKNOWN'),
            })
            
            # 5. ROIä¿¡æ¯
            roi_info = result_data.get('roi_info', {})
            signal_roi = roi_info.get('signal', {})
            background_roi = roi_info.get('background', {})
            
            if signal_roi:
                signal_stats = signal_roi.get('statistics', {})
                scan_info.update({
                    'signal_mean': signal_stats.get('mean', 0.0),
                    'signal_std': signal_stats.get('std', 0.0),
                    'signal_cv': signal_stats.get('std', 0.0) / signal_stats.get('mean', 1.0) 
                                if signal_stats.get('mean', 0) > 0 else 0.0,
                })
            
            if background_roi:
                bg_stats = background_roi.get('statistics', {})
                scan_info.update({
                    'background_mean': bg_stats.get('mean', 0.0),
                    'background_std': bg_stats.get('std', 0.0),
                    'noise_uniformity_cv': result_data.get('quality_assessment', {})
                                       .get('noise_uniformity', {}).get('cv', 0.0),
                })
            
            return scan_info
            
        except Exception as e:
            self.log(f"   æå–æ‰«æä¿¡æ¯å¤±è´¥ {result_file}: {e}")
            return None
    
    def _generate_executive_summary(self, df: pd.DataFrame, report_dir: Path):
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        summary_dir = report_dir / "00_executive_summary"
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        completed_df = df[df['analysis_status'] == 'COMPLETED'].copy()
        failed_df = df[df['analysis_status'] != 'COMPLETED'].copy()
        
        total_scans = len(df)
        successful_scans = len(completed_df)
        failed_scans = len(failed_df)
        success_rate = successful_scans / total_scans * 100 if total_scans > 0 else 0
        
        # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡ç»Ÿè®¡
        if len(completed_df) > 0:
            snr_stats = {
                'mean': completed_df['snr_corrected'].mean(),
                'median': completed_df['snr_corrected'].median(),
                'std': completed_df['snr_corrected'].std(),
                'min': completed_df['snr_corrected'].min(),
                'max': completed_df['snr_corrected'].max(),
                'q1': completed_df['snr_corrected'].quantile(0.25),
                'q3': completed_df['snr_corrected'].quantile(0.75),
            }
            
            cnr_stats = {
                'mean': completed_df['cnr_value'].mean() if 'cnr_value' in completed_df.columns else 0,
                'median': completed_df['cnr_value'].median() if 'cnr_value' in completed_df.columns else 0,
            }
            
            quality_stats = {
                'mean': completed_df['quality_score_total'].mean() if 'quality_score_total' in completed_df.columns else 0,
                'median': completed_df['quality_score_total'].median() if 'quality_score_total' in completed_df.columns else 0,
            }
            
            confidence_stats = {
                'mean': completed_df['confidence_score'].mean() if 'confidence_score' in completed_df.columns else 0,
                'median': completed_df['confidence_score'].median() if 'confidence_score' in completed_df.columns else 0,
            }
        else:
            snr_stats = cnr_stats = quality_stats = confidence_stats = {}
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_file = summary_dir / f"executive_summary_{self.report_timestamp}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            self._write_executive_summary_content(f, df, completed_df, failed_df, 
                                                 total_scans, successful_scans, failed_scans, 
                                                 success_rate, snr_stats, cnr_stats, 
                                                 quality_stats, confidence_stats)
        
        # ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
        txt_file = summary_dir / f"quick_summary_{self.report_timestamp}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            self._write_quick_summary(f, total_scans, successful_scans, success_rate, snr_stats)
    
    def _write_executive_summary_content(self, f, df, completed_df, failed_df, 
                                        total_scans, successful_scans, failed_scans,
                                        success_rate, snr_stats, cnr_stats,
                                        quality_stats, confidence_stats):
        """å†™å…¥æ‰§è¡Œæ‘˜è¦å†…å®¹"""
        duration = (self.end_time - self.start_time).total_seconds() if self.end_time else 0
        
        f.write(f"""# MRI AutoQA æ‰¹é‡åˆ†ææ‰§è¡Œæ‘˜è¦
## æŠ¥å‘ŠID: {self.report_id}

### ğŸ“‹ æŠ¥å‘Šä¿¡æ¯
| é¡¹ç›® | å†…å®¹ |
|------|------|
| **æŠ¥å‘Šç”Ÿæˆæ—¶é—´** | {self.start_time.strftime('%Y-%m-%d %H:%M:%S')} |
| **åˆ†ææ—¶é•¿** | {duration:.0f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ) |
| **æ‰«ææ€»æ•°** | {total_scans} |
| **æˆåŠŸåˆ†æ** | {successful_scans} ({success_rate:.1f}%) |
| **åˆ†æå¤±è´¥** | {failed_scans} |
| **è½¯ä»¶ç‰ˆæœ¬** | MRI_AutoQA_v2.0 |
| **ç®—æ³•ç‰ˆæœ¬** | single_image_v3_rayleigh |

### ğŸ¯ æ ¸å¿ƒæŒ‡æ ‡æ¦‚è§ˆ
| æŒ‡æ ‡ | å¹³å‡å€¼ | ä¸­ä½æ•° | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ |
|------|--------|--------|--------|--------|--------|
| **æ ¡æ­£SNR** | {snr_stats.get('mean', 0):.1f} | {snr_stats.get('median', 0):.1f} | {snr_stats.get('std', 0):.1f} | {snr_stats.get('min', 0):.1f} | {snr_stats.get('max', 0):.1f} |
| **CNRå€¼** | {cnr_stats.get('mean', 0):.2f} | {cnr_stats.get('median', 0):.2f} | - | - | - |
| **è´¨é‡æ€»åˆ†** | {quality_stats.get('mean', 0):.3f} | {quality_stats.get('median', 0):.3f} | - | - | - |
| **ç½®ä¿¡åº¦** | {confidence_stats.get('mean', 0):.3f} | {confidence_stats.get('median', 0):.3f} | - | - | - |

### ğŸ“ˆ è´¨é‡è¯„çº§åˆ†å¸ƒ
""")
        
        # SNRè¯„çº§åˆ†å¸ƒ
        if 'snr_rating' in completed_df.columns:
            rating_dist = completed_df['snr_rating'].value_counts().sort_index()
            f.write("#### SNRè¯„çº§åˆ†å¸ƒ\n")
            f.write("| è¯„çº§ | æ‰«ææ•° | ç™¾åˆ†æ¯” |\n")
            f.write("|------|--------|--------|\n")
            for rating, count in rating_dist.items():
                percentage = count / successful_scans * 100
                f.write(f"| {rating} | {count} | {percentage:.1f}% |\n")
            f.write("\n")
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        if 'algorithm_confidence' in completed_df.columns:
            conf_dist = completed_df['algorithm_confidence'].value_counts().sort_index()
            f.write("#### ç®—æ³•ç½®ä¿¡åº¦åˆ†å¸ƒ\n")
            f.write("| ç½®ä¿¡åº¦ç­‰çº§ | æ‰«ææ•° | ç™¾åˆ†æ¯” |\n")
            f.write("|------------|--------|--------|\n")
            for level, count in conf_dist.items():
                percentage = count / successful_scans * 100
                f.write(f"| {level} | {count} | {percentage:.1f}% |\n")
            f.write("\n")
        
        # é—®é¢˜æ‰«æ
        if failed_scans > 0:
            f.write("### âš ï¸ é—®é¢˜æ‰«æ\n")
            f.write("| æ‰«æID | é”™è¯¯ç±»å‹ | é”™è¯¯ä¿¡æ¯ |\n")
            f.write("|--------|----------|----------|\n")
            for _, row in failed_df.iterrows():
                scan_id = f"{row.get('patient_id', '')}/{row.get('scan_name', '')}"
                error_type = row.get('error_type', 'UNKNOWN')
                error_msg = str(row.get('error_message', ''))[:50]
                f.write(f"| {scan_id} | {error_type} | {error_msg}... |\n")
        
        # å»ºè®®è¡ŒåŠ¨
        f.write("\n### ğŸš€ å»ºè®®è¡ŒåŠ¨\n")
        f.write("1. **æ£€æŸ¥ä½è´¨é‡æ‰«æ**ï¼šæŸ¥çœ‹è´¨é‡åˆ†<0.7çš„æ‰«æ\n")
        f.write("2. **å®¡æŸ¥å¤±è´¥æ‰«æ**ï¼šåˆ†æå¤±è´¥åŸå› å¹¶é‡æ–°å¤„ç†\n")
        f.write("3. **ä¼˜åŒ–é‡‡é›†å‚æ•°**ï¼šå…³æ³¨ä½SNRæ‰«æçš„é‡‡é›†è®¾ç½®\n")
        f.write("4. **å®šæœŸè´¨é‡ç›‘æ§**ï¼šå»ºç«‹è´¨é‡åŸºå‡†å¹¶æŒç»­è·Ÿè¸ª\n")
        
        f.write(f"\n---\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    def _write_quick_summary(self, f, total_scans, successful_scans, success_rate, snr_stats):
        """å†™å…¥å¿«é€Ÿæ‘˜è¦"""
        f.write(f"MRI AutoQA å¿«é€Ÿæ‘˜è¦\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æŠ¥å‘ŠID: {self.report_id}\n")
        f.write(f"\næ ¸å¿ƒç»Ÿè®¡:\n")
        f.write(f"  æ‰«ææ€»æ•°: {total_scans}\n")
        f.write(f"  æˆåŠŸåˆ†æ: {successful_scans} ({success_rate:.1f}%)\n")
        if snr_stats:
            f.write(f"  å¹³å‡SNR: {snr_stats.get('mean', 0):.1f}\n")
            f.write(f"  SNRèŒƒå›´: {snr_stats.get('min', 0):.1f} - {snr_stats.get('max', 0):.1f}\n")
    
    def _generate_detailed_data_files(self, df: pd.DataFrame, report_dir: Path):
        """ç”Ÿæˆè¯¦ç»†æ•°æ®æ–‡ä»¶"""
        data_dir = report_dir / "01_detailed_data"
        
        # 1. å®Œæ•´æ•°æ®CSV
        csv_file = data_dir / f"detailed_results_{self.report_timestamp}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8')
        self.log(f"   âœ“ è¯¦ç»†ç»“æœCSV: {csv_file.name}")
        
        # 2. æˆåŠŸæ‰«ææ•°æ®
        completed_df = df[df['analysis_status'] == 'COMPLETED'].copy()
        if len(completed_df) > 0:
            completed_csv = data_dir / f"successful_scans_{self.report_timestamp}.csv"
            completed_df.to_csv(completed_csv, index=False, encoding='utf-8')
            self.log(f"   âœ“ æˆåŠŸæ‰«æCSV: {completed_csv.name}")
        
        # 3. å¤±è´¥æ‰«ææ•°æ®
        failed_df = df[df['analysis_status'] != 'COMPLETED'].copy()
        if len(failed_df) > 0:
            failed_csv = data_dir / f"failed_scans_{self.report_timestamp}.csv"
            failed_df.to_csv(failed_csv, index=False, encoding='utf-8')
            self.log(f"   âœ“ å¤±è´¥æ‰«æCSV: {failed_csv.name}")
        
        # 4. æ‰«æå‚æ•°æ¸…å•
        #self._generate_scan_parameters_summary(data_dir, df)
        
        # 5. å­—æ®µè¯´æ˜æ–‡æ¡£
        fields_doc = data_dir / f"data_fields_{self.report_timestamp}.md"
        with open(fields_doc, 'w', encoding='utf-8') as f:
            f.write("# æ•°æ®å­—æ®µè¯´æ˜\n\n")
            f.write("| å­—æ®µå | è¯´æ˜ | ç±»å‹ | ç¤ºä¾‹ |\n")
            f.write("|--------|------|------|------|\n")
            fields = [
                ('scan_id', 'æ‰«ææ ‡è¯†ç¬¦', 'å­—ç¬¦ä¸²', 'p001/T1_1'),
                ('patient_id', 'æ‚£è€…ID', 'å­—ç¬¦ä¸²', '01620360'),
                ('scan_name', 'æ‰«æåç§°', 'å­—ç¬¦ä¸²', 'T1_tse_sag_320'),
                ('anatomical_region', 'è§£å‰–åŒºåŸŸ', 'å­—ç¬¦ä¸²', 'lumbar'),
                ('sequence_type', 'åºåˆ—ç±»å‹', 'å­—ç¬¦ä¸²', 't1'),
                ('field_strength', 'ç£åœºå¼ºåº¦', 'å­—ç¬¦ä¸²', '1.5t'),
                ('snr_corrected', 'æ ¡æ­£åSNR', 'æµ®ç‚¹æ•°', '24.93'),
                ('snr_rating', 'SNRè¯„çº§', 'å­—ç¬¦ä¸²', 'GOOD'),
                ('cnr_value', 'CNRå€¼', 'æµ®ç‚¹æ•°', '5.79'),
                ('quality_score_total', 'è´¨é‡æ€»åˆ†', 'æµ®ç‚¹æ•°', '0.88'),
                ('confidence_score', 'ç½®ä¿¡åº¦åˆ†æ•°', 'æµ®ç‚¹æ•°', '0.64'),
                ('algorithm_confidence', 'ç½®ä¿¡åº¦ç­‰çº§', 'å­—ç¬¦ä¸²', 'MEDIUM'),
                ('analysis_status', 'åˆ†æçŠ¶æ€', 'å­—ç¬¦ä¸²', 'COMPLETED'),
            ]
            for field_name, description, field_type, example in fields:
                f.write(f"| {field_name} | {description} | {field_type} | {example} |\n")
        
        self.log(f"   âœ“ å­—æ®µè¯´æ˜æ–‡æ¡£")
    
   

    def _generate_statistical_reports(self, df: pd.DataFrame, report_dir: Path):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        stats_dir = report_dir / "03_quality_analysis"
        completed_df = df[df['analysis_status'] == 'COMPLETED'].copy()
        
        if len(completed_df) == 0:
            return
        
        # 1. åŸºç¡€ç»Ÿè®¡æŠ¥å‘Š
        stats_file = stats_dir / f"statistical_report_{self.report_timestamp}.md"
        with open(stats_file, 'w', encoding='utf-8') as f:
            self._write_statistical_report(f, completed_df)
        
        # 2. æŒ‰è§£å‰–åŒºåŸŸç»Ÿè®¡
        if 'anatomical_region' in completed_df.columns:
            anatomy_stats = self._calculate_anatomy_statistics(completed_df)
            anatomy_file = stats_dir / f"anatomy_statistics_{self.report_timestamp}.csv"
            anatomy_stats.to_csv(anatomy_file, encoding='utf-8')
        
        # 3. æŒ‰åºåˆ—ç±»å‹ç»Ÿè®¡
        if 'sequence_type' in completed_df.columns:
            sequence_stats = self._calculate_sequence_statistics(completed_df)
            sequence_file = stats_dir / f"sequence_statistics_{self.report_timestamp}.csv"
            sequence_stats.to_csv(sequence_file, encoding='utf-8')
    
    def _write_statistical_report(self, f, completed_df):
        """å†™å…¥ç»Ÿè®¡æŠ¥å‘Š"""
        total_scans = len(completed_df)
        
        f.write(f"""# MRIè´¨é‡ç»Ÿè®¡æŠ¥å‘Š
## æŠ¥å‘ŠID: {self.report_id}
## ç»Ÿè®¡æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

### ä¸€ã€æ€»ä½“ç»Ÿè®¡
- **ç»Ÿè®¡æ‰«ææ•°**: {total_scans}
- **ç»Ÿè®¡æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

### äºŒã€å…³é”®æŒ‡æ ‡ç»Ÿè®¡
""")
        
        # SNRç»Ÿè®¡
        if 'snr_corrected' in completed_df.columns:
            f.write("#### 1. SNRç»Ÿè®¡\n")
            f.write("```\n")
            f.write(f"  å¹³å‡å€¼: {completed_df['snr_corrected'].mean():.2f}\n")
            f.write(f"  ä¸­ä½æ•°: {completed_df['snr_corrected'].median():.2f}\n")
            f.write(f"  æ ‡å‡†å·®: {completed_df['snr_corrected'].std():.2f}\n")
            f.write(f"  æœ€å°å€¼: {completed_df['snr_corrected'].min():.2f}\n")
            f.write(f"  æœ€å¤§å€¼: {completed_df['snr_corrected'].max():.2f}\n")
            f.write(f"  25åˆ†ä½æ•°: {completed_df['snr_corrected'].quantile(0.25):.2f}\n")
            f.write(f"  75åˆ†ä½æ•°: {completed_df['snr_corrected'].quantile(0.75):.2f}\n")
            f.write("```\n\n")
        
        # CNRç»Ÿè®¡
        if 'cnr_value' in completed_df.columns:
            f.write("#### 2. CNRç»Ÿè®¡\n")
            f.write("```\n")
            f.write(f"  å¹³å‡å€¼: {completed_df['cnr_value'].mean():.2f}\n")
            f.write(f"  ä¸­ä½æ•°: {completed_df['cnr_value'].median():.2f}\n")
            f.write(f"  æ ‡å‡†å·®: {completed_df['cnr_value'].std():.2f}\n")
            f.write("```\n\n")
        
        # è´¨é‡åˆ†ç»Ÿè®¡
        if 'quality_score_total' in completed_df.columns:
            f.write("#### 3. è´¨é‡åˆ†ç»Ÿè®¡\n")
            f.write("```\n")
            f.write(f"  å¹³å‡å€¼: {completed_df['quality_score_total'].mean():.3f}\n")
            f.write(f"  ä¸­ä½æ•°: {completed_df['quality_score_total'].median():.3f}\n")
            f.write(f"  æ ‡å‡†å·®: {completed_df['quality_score_total'].std():.3f}\n")
            f.write("```\n\n")
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        if 'confidence_score' in completed_df.columns:
            f.write("#### 4. ç½®ä¿¡åº¦ç»Ÿè®¡\n")
            f.write("```\n")
            f.write(f"  å¹³å‡å€¼: {completed_df['confidence_score'].mean():.3f}\n")
            f.write(f"  ä¸­ä½æ•°: {completed_df['confidence_score'].median():.3f}\n")
            f.write(f"  æ ‡å‡†å·®: {completed_df['confidence_score'].std():.3f}\n")
            f.write("```\n")
    
    def _calculate_anatomy_statistics(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—è§£å‰–åŒºåŸŸç»Ÿè®¡"""
        if 'anatomical_region' not in df.columns:
            return pd.DataFrame()
        
        stats_list = []
        
        for region in df['anatomical_region'].unique():
            region_df = df[df['anatomical_region'] == region]
            if len(region_df) == 0:
                continue
            
            stats = {
                'anatomical_region': region,
                'scan_count': len(region_df),
                'snr_mean': region_df['snr_corrected'].mean() if 'snr_corrected' in region_df.columns else 0,
                'snr_median': region_df['snr_corrected'].median() if 'snr_corrected' in region_df.columns else 0,
                'cnr_mean': region_df['cnr_value'].mean() if 'cnr_value' in region_df.columns else 0,
                'quality_mean': region_df['quality_score_total'].mean() if 'quality_score_total' in region_df.columns else 0,
                'confidence_mean': region_df['confidence_score'].mean() if 'confidence_score' in region_df.columns else 0,
            }
            stats_list.append(stats)
        
        return pd.DataFrame(stats_list)
    
    def _calculate_sequence_statistics(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—åºåˆ—ç±»å‹ç»Ÿè®¡"""
        if 'sequence_type' not in df.columns:
            return pd.DataFrame()
        
        stats_list = []
        
        for seq_type in df['sequence_type'].unique():
            seq_df = df[df['sequence_type'] == seq_type]
            if len(seq_df) == 0:
                continue
            
            stats = {
                'sequence_type': seq_type,
                'scan_count': len(seq_df),
                'snr_mean': seq_df['snr_corrected'].mean() if 'snr_corrected' in seq_df.columns else 0,
                'snr_median': seq_df['snr_corrected'].median() if 'snr_corrected' in seq_df.columns else 0,
                'cnr_mean': seq_df['cnr_value'].mean() if 'cnr_value' in seq_df.columns else 0,
                'quality_mean': seq_df['quality_score_total'].mean() if 'quality_score_total' in seq_df.columns else 0,
                'confidence_mean': seq_df['confidence_score'].mean() if 'confidence_score' in seq_df.columns else 0,
            }
            stats_list.append(stats)
        
        return pd.DataFrame(stats_list)
    
    def _generate_quality_analysis(self, df: pd.DataFrame, report_dir: Path):
        """ç”Ÿæˆè´¨é‡åˆ†ææŠ¥å‘Š"""
        analysis_dir = report_dir / "03_quality_analysis"
        completed_df = df[df['analysis_status'] == 'COMPLETED'].copy()
        
        if len(completed_df) == 0:
            return
        
        # 1. è¯†åˆ«é—®é¢˜æ‰«æ
        problem_scans = self._identify_problem_scans(completed_df)
        if len(problem_scans) > 0:
            problem_file = analysis_dir / f"problem_scans_{self.report_timestamp}.csv"
            problem_scans.to_csv(problem_file, index=False, encoding='utf-8')
            
            # ç”Ÿæˆé—®é¢˜æ‰«ææŠ¥å‘Š
            problem_report = analysis_dir / f"problem_analysis_{self.report_timestamp}.md"
            with open(problem_report, 'w', encoding='utf-8') as f:
                self._write_problem_analysis(f, problem_scans)
        
        # 2. è¯†åˆ«ä½ç½®ä¿¡åº¦æ‰«æ
        if 'confidence_score' in completed_df.columns:
            low_confidence = completed_df[completed_df['confidence_score'] < 0.6].copy()
            if len(low_confidence) > 0:
                low_conf_file = analysis_dir / f"low_confidence_scans_{self.report_timestamp}.csv"
                low_confidence.to_csv(low_conf_file, index=False, encoding='utf-8')
    
    def _identify_problem_scans(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¯†åˆ«é—®é¢˜æ‰«æ"""
        problem_filters = []
        
        # ä½SNRæ‰«æ
        if 'snr_corrected' in df.columns:
            low_snr = df['snr_corrected'] < 15
            problem_filters.append(low_snr)
        
        # ä½è´¨é‡åˆ†æ‰«æ
        if 'quality_score_total' in df.columns:
            low_quality = df['quality_score_total'] < 0.7
            problem_filters.append(low_quality)
        
        # ä½CNRæ‰«æ
        if 'cnr_value' in df.columns:
            low_cnr = df['cnr_value'] < 3.0
            problem_filters.append(low_cnr)
        
        # ç»„åˆæ‰€æœ‰ç­›é€‰æ¡ä»¶
        if problem_filters:
            problem_mask = problem_filters[0]
            for filter_mask in problem_filters[1:]:
                problem_mask = problem_mask | filter_mask
            
            problem_df = df[problem_mask].copy()
            
            # æ·»åŠ é—®é¢˜ç±»å‹åˆ—
            problem_types = []
            for _, row in problem_df.iterrows():
                types = []
                if row.get('snr_corrected', 100) < 15:
                    types.append('ä½SNR')
                if row.get('quality_score_total', 1) < 0.7:
                    types.append('ä½è´¨é‡åˆ†')
                if row.get('cnr_value', 10) < 3.0:
                    types.append('ä½CNR')
                problem_types.append('ã€'.join(types))
            
            problem_df['problem_type'] = problem_types
            return problem_df
        
        return pd.DataFrame()
    
    def _write_problem_analysis(self, f, problem_df):
        """å†™å…¥é—®é¢˜åˆ†ææŠ¥å‘Š"""
        f.write(f"""# é—®é¢˜æ‰«æåˆ†ææŠ¥å‘Š
## æŠ¥å‘ŠID: {self.report_id}
## ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

### æ¦‚è¿°
- **æ€»é—®é¢˜æ‰«ææ•°**: {len(problem_df)}
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

### é—®é¢˜æ‰«æè¯¦æƒ…
| æ‰«æID | æ‚£è€…ID | é—®é¢˜ç±»å‹ | SNR | CNR | è´¨é‡åˆ† | ç½®ä¿¡åº¦ | å»ºè®® |
|--------|--------|----------|-----|-----|--------|--------|------|
""")
        
        for _, row in problem_df.iterrows():
            scan_id = f"{row.get('patient_id', '')}/{row.get('scan_name', '')}"
            patient_id = row.get('patient_id', '')
            problem_type = row.get('problem_type', 'æœªçŸ¥')
            snr = row.get('snr_corrected', 0)
            cnr = row.get('cnr_value', 0)
            quality = row.get('quality_score_total', 0)
            confidence = row.get('confidence_score', 0)
            
            # æ ¹æ®é—®é¢˜ç±»å‹ç»™å‡ºå»ºè®®
            if 'ä½SNR' in problem_type:
                suggestion = 'æ£€æŸ¥é‡‡é›†å‚æ•°ï¼Œè€ƒè™‘é‡æ–°é‡‡é›†'
            elif 'ä½è´¨é‡åˆ†' in problem_type:
                suggestion = 'å…¨é¢æ£€æŸ¥å›¾åƒè´¨é‡'
            elif 'ä½CNR' in problem_type:
                suggestion = 'ä¼˜åŒ–åºåˆ—å‚æ•°'
            else:
                suggestion = 'éœ€è¦è¿›ä¸€æ­¥åˆ†æ'
            
            f.write(f"| {scan_id} | {patient_id} | {problem_type} | {snr:.1f} | {cnr:.2f} | {quality:.3f} | {confidence:.3f} | {suggestion} |\n")
        
        f.write("\n### æ”¹è¿›å»ºè®®\n")
        f.write("1. **ä½SNRæ‰«æ**ï¼šæ£€æŸ¥é‡‡é›†æ—¶é—´ã€çº¿åœˆä½ç½®ã€åºåˆ—å‚æ•°\n")
        f.write("2. **ä½è´¨é‡åˆ†æ‰«æ**ï¼šå…¨é¢è¯„ä¼°å›¾åƒè´¨é‡ï¼Œæ£€æŸ¥ä¼ªå½±\n")
        f.write("3. **ä½CNRæ‰«æ**ï¼šä¼˜åŒ–åºåˆ—å¯¹æ¯”åº¦å‚æ•°\n")
        f.write("4. **å®šæœŸå¤æŸ¥**ï¼šå»ºç«‹è´¨é‡ç›‘æ§æœºåˆ¶\n")
    
    def _generate_technical_appendix(self, df: pd.DataFrame, report_dir: Path):
        """ç”ŸæˆæŠ€æœ¯é™„å½•"""
        tech_dir = report_dir / "04_technical_appendix"
        
        # 1. å¤„ç†ç»Ÿè®¡
        stats_file = tech_dir / f"processing_stats_{self.report_timestamp}.json"
        stats = {
            'report_info': {
                'report_id': self.report_id,
                'timestamp': self.report_timestamp,
                'generated_at': datetime.now().isoformat(),
                'total_scans': len(df),
                'successful_scans': len(df[df['analysis_status'] == 'COMPLETED']),
                'failed_scans': len(df[df['analysis_status'] != 'COMPLETED']),
            },
            'system_info': {
                'python_version': sys.version,
                'pandas_version': pd.__version__,
                'numpy_version': np.__version__ if hasattr(np, '__version__') else 'unknown',
            }
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        # 2. å­—æ®µæ˜ å°„æ–‡æ¡£
        mapping_file = tech_dir / f"field_mapping_{self.report_timestamp}.md"
        with open(mapping_file, 'w', encoding='utf-8') as f:
            f.write("# å­—æ®µæ˜ å°„æ–‡æ¡£\n\n")
            f.write("| JSONå­—æ®µ | æŠ¥å‘Šå­—æ®µ | è¯´æ˜ |\n")
            f.write("|----------|----------|------|\n")
            mappings = [
                ('analysis_info.scan_id', 'scan_id', 'æ‰«ææ ‡è¯†ç¬¦'),
                ('acquisition.patient_id', 'patient_id', 'æ‚£è€…ID'),
                ('acquisition.scan_name', 'scan_name', 'æ‰«æåç§°'),
                ('acquisition.anatomical_region', 'anatomical_region', 'è§£å‰–åŒºåŸŸ'),
                ('acquisition.sequence_type', 'sequence_type', 'åºåˆ—ç±»å‹'),
                ('acquisition.field_strength', 'field_strength', 'ç£åœºå¼ºåº¦'),
                ('snr_results.recommended.snr', 'snr_corrected', 'æ ¡æ­£åSNR'),
                ('quality_assessment.snr_rating.level', 'snr_rating', 'SNRè¯„çº§'),
                ('quality_assessment.cnr_analysis.best_cnr.cnr_value', 'cnr_value', 'CNRå€¼'),
                ('quality_assessment.quality_scores.total_score', 'quality_score_total', 'è´¨é‡æ€»åˆ†'),
                ('quality_assessment.overall_confidence.score', 'confidence_score', 'ç½®ä¿¡åº¦åˆ†æ•°'),
                ('quality_assessment.overall_confidence.level', 'algorithm_confidence', 'ç½®ä¿¡åº¦ç­‰çº§'),
            ]
            for json_field, report_field, description in mappings:
                f.write(f"| `{json_field}` | `{report_field}` | {description} |\n")
    
    def _create_report_index(self, report_dir: Path, df: pd.DataFrame):
        """åˆ›å»ºæŠ¥å‘Šç´¢å¼•æ–‡ä»¶"""
        index_file = report_dir / f"REPORT_INDEX_{self.report_timestamp}.md"
        
        completed_df = df[df['analysis_status'] == 'COMPLETED'].copy()
        total_scans = len(df)
        successful_scans = len(completed_df)
        success_rate = successful_scans / total_scans * 100 if total_scans > 0 else 0
        
        with open(index_file, 'w', encoding='utf-8') as f:
            f.write(f"""# MRI AutoQA æ‰¹é‡åˆ†ææŠ¥å‘Šç´¢å¼•
## æŠ¥å‘ŠID: {self.report_id}

### ğŸ“Š å¿«é€Ÿç»Ÿè®¡
- **æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}
- **æ‰«ææ€»æ•°**: {total_scans}
- **æˆåŠŸåˆ†æ**: {successful_scans} ({success_rate:.1f}%)
- **åˆ†æå¤±è´¥**: {total_scans - successful_scans}
""")
            f.write("\n### ğŸ“Š æ‰«æå‚æ•°æ¸…å•è¯´æ˜\n")
            f.write("- **æ–‡ä»¶**: `01_detailed_data/scan_parameters_summary_*.csv`\n")
            f.write("- **å†…å®¹**: æ‰€æœ‰æ‰«æçš„å®Œæ•´å‚æ•°æ¸…å•\n")
            f.write("- **scan_idæ ¼å¼**: ä¸è´¨é‡åˆ†æç»“æœä¸€è‡´ï¼ˆä¼˜å…ˆä½¿ç”¨å¤„ç†åçš„IDæ ¼å¼ï¼‰\n")
            f.write("- **åŒ…å«**: å·²åˆ†æå’Œæœªåˆ†æçš„æ‰«æå‚æ•°\n")
            f.write("- **å…³è”å­—æ®µ**: `original_scan_id` å­—æ®µä¿æŒåŸå§‹DICOM IDæ ¼å¼\n")
            if len(completed_df) > 0 and 'snr_corrected' in completed_df.columns:
                avg_snr = completed_df['snr_corrected'].mean()
                f.write(f"- **å¹³å‡æ ¡æ­£SNR**: {avg_snr:.1f}\n")
            
            f.write("\n### ğŸ“ æ–‡ä»¶ç»“æ„\n")
            f.write("```\n")
            self._print_directory_tree(report_dir, f, max_depth=2)
            f.write("```\n\n")
            
            f.write("### ğŸ“„ æ–‡ä»¶è¯´æ˜\n")
            f.write("| æ–‡ä»¶/ç›®å½• | è¯´æ˜ |\n")
            f.write("|-----------|------|\n")
            f.write("| `00_executive_summary/` | æ‰§è¡Œæ‘˜è¦ï¼Œé€‚åˆç®¡ç†è€…é˜…è¯» |\n")
            f.write("| `01_detailed_data/` | è¯¦ç»†æ•°æ®æ–‡ä»¶ï¼Œä¾›æ•°æ®åˆ†æ |\n")
            f.write("| `02_visualizations/` | å¯è§†åŒ–å›¾è¡¨ï¼ˆå¦‚ç”Ÿæˆï¼‰ |\n")
            f.write("| `03_quality_analysis/` | è´¨é‡åˆ†æå’Œç»Ÿè®¡æŠ¥å‘Š |\n")
            f.write("| `04_technical_appendix/` | æŠ€æœ¯é™„å½•å’Œå…ƒæ•°æ® |\n")
            f.write(f"| `REPORT_INDEX_{self.report_timestamp}.md` | æœ¬ç´¢å¼•æ–‡ä»¶ |\n")
            
            f.write("\n### ğŸš€ ä½¿ç”¨æŒ‡å—\n")
            f.write("1. **å¿«é€Ÿæµè§ˆ**ï¼šæŸ¥çœ‹ `00_executive_summary/executive_summary_*.md`\n")
            f.write("2. **æ•°æ®åˆ†æ**ï¼šä½¿ç”¨ `01_detailed_data/detailed_results_*.csv`\n")
            f.write("3. **æ‰«æå‚æ•°**ï¼šæŸ¥çœ‹ `01_detailed_data/scan_parameters_summary_*.csv`\n")
            f.write("4. **é—®é¢˜æ’æŸ¥**ï¼šæŸ¥çœ‹ `03_quality_analysis/problem_scans_*.csv`\n")
            f.write("5. **æŠ€æœ¯å‚è€ƒ**ï¼šæŸ¥é˜… `04_technical_appendix/` ä¸­çš„æ–‡ä»¶\n")
            
            f.write("\n### ğŸ“§ æŠ¥å‘Šä¿¡æ¯\n")
            f.write(f"- **å”¯ä¸€æ ‡è¯†**: {self.report_id}\n")
            f.write(f"- **æ—¶é—´æˆ³**: {self.report_timestamp}\n")
            f.write(f"- **æ•°æ®ç‰ˆæœ¬**: single_image_v3_rayleigh\n")
            f.write(f"- **ç”Ÿæˆå·¥å…·**: MRI_AutoQA_v2.0\n")
            
            f.write(f"\n---\n*ç´¢å¼•ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    def _generate_visualizations(self) -> bool:
        """ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š - ä¿®å¤ç‰ˆ"""
        self.log("\nğŸ¨ æ£€æŸ¥å¹¶ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
        
        # æ£€æŸ¥skimageæ˜¯å¦å¯ç”¨
        if not self.modules_available.get('skimage', False):
            self.log("âš ï¸  skimageæ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–ç”Ÿæˆ")
            self.log("   å¦‚éœ€å¯è§†åŒ–åŠŸèƒ½ï¼Œè¯·å®‰è£…: pip install scikit-image")
            return True
        
        # æ£€æŸ¥å¯è§†åŒ–æ¨¡å—æ˜¯å¦å¯ç”¨
        if not self.modules_available.get('visualization', False):
            self.log("âš ï¸  å¯è§†åŒ–æ¨¡å—æ–‡ä»¶ç¼ºå¤±ï¼Œè·³è¿‡å¯è§†åŒ–ç”Ÿæˆ")
            self.log("   ç¡®ä¿ visualization.py å’Œ visualization_config.py å­˜åœ¨")
            return True
        
        try:
            # å°è¯•å¯¼å…¥å¯è§†åŒ–æ¨¡å—
            from visualization import create_visualization_for_scan
            
            # 1. æŸ¥æ‰¾æ‰€æœ‰æˆåŠŸçš„åˆ†æç»“æœ
            result_files = list(self.results_dir.rglob("**/qa_report.json"))
            
            if not result_files:
                self.log("   æœªæ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶")
                return True
            
            completed_results = []
            for result_file in result_files:
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result_data = json.load(f)
                    
                    if result_data.get('analysis_status') == 'COMPLETED':
                        completed_results.append((result_file, result_data))
                except:
                    continue
            
            if not completed_results:
                self.log("   æ²¡æœ‰æˆåŠŸçš„åˆ†æç»“æœ")
                return True
            
            self.log(f"   æ‰¾åˆ° {len(completed_results)} ä¸ªæˆåŠŸåˆ†æç»“æœ")
            
            # 2. æ£€æŸ¥å“ªäº›æ‰«æå·²ç»ç”Ÿæˆäº†å¯è§†åŒ–
            need_viz = []
            already_have_viz = []
            
            for result_file, result_data in completed_results:
                result_dir = result_file.parent
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å¯è§†åŒ–æ–‡ä»¶
                viz_files = list(result_dir.glob("*.png"))
                if viz_files:
                    already_have_viz.append((result_dir, viz_files))
                else:
                    need_viz.append((result_file, result_data, result_dir))
            
            # 3. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self.log(f"   å¯è§†åŒ–çŠ¶æ€ç»Ÿè®¡:")
            self.log(f"     â€¢ å·²ç”Ÿæˆå¯è§†åŒ–: {len(already_have_viz)} ä¸ªæ‰«æ")
            self.log(f"     â€¢ éœ€ç”Ÿæˆå¯è§†åŒ–: {len(need_viz)} ä¸ªæ‰«æ")
            
            # æ˜¾ç¤ºéƒ¨åˆ†å·²å­˜åœ¨çš„å¯è§†åŒ–æ–‡ä»¶
            if already_have_viz and self.verbose:
                self.log(f"   å·²å­˜åœ¨çš„å¯è§†åŒ–æ–‡ä»¶ï¼ˆå‰5ä¸ªï¼‰:")
                for result_dir, viz_files in already_have_viz[:5]:
                    self.log(f"     - {result_dir.name}: {[f.name for f in viz_files]}")
                if len(already_have_viz) > 5:
                    self.log(f"     ... å’Œ {len(already_have_viz) - 5} ä¸ªæ›´å¤šæ‰«æ")
            
            # 4. å¤„ç†å•æ‰«æå¯è§†åŒ–
            if not need_viz:
                self.log("   æ‰€æœ‰æ‰«æå·²å…·å¤‡å•æ‰«æå¯è§†åŒ–æ–‡ä»¶")
                # ç»§ç»­æ‰§è¡Œæ‰¹é‡å¯è§†åŒ–
            else:
                self.log(f"\n   å¼€å§‹ä¸º {len(need_viz)} ä¸ªæ‰«æç”Ÿæˆå•æ‰«æå¯è§†åŒ–...")
                
                success_count = 0
                failed_count = 0
                
                for i, (result_file, result_data, result_dir) in enumerate(need_viz, 1):
                    try:
                        # è¿›åº¦æ˜¾ç¤º
                        if self.verbose and i % 10 == 0:
                            self.log(f"    å¤„ç†ä¸­: {i}/{len(need_viz)} ({i/len(need_viz)*100:.1f}%)")
                        
                        # æŸ¥æ‰¾å¯¹åº”çš„NIfTIæ–‡ä»¶
                        nifti_file = result_dir / "scan.nii.gz"
                        if not nifti_file.exists():
                            # å°è¯•ä»åŸå§‹è·¯å¾„æŸ¥æ‰¾
                            scan_id = result_data.get('analysis_info', {}).get('scan_id', '')
                            if '/' in scan_id:
                                patient_id, scan_name = scan_id.split('/', 1)
                                nifti_file = self.paths['input_data'] / patient_id / scan_name / "scan.nii.gz"
                        
                        if not nifti_file.exists():
                            self.log(f"    è­¦å‘Š: æ— æ³•æ‰¾åˆ°NIfTIæ–‡ä»¶ï¼Œè·³è¿‡ {result_dir.name}")
                            failed_count += 1
                            continue
                        
                        # åŠ è½½å›¾åƒæ•°æ®
                        import nibabel as nib
                        img = nib.load(nifti_file).get_fdata()
                        
                        # æå–ä¸­é—´åˆ‡ç‰‡
                        if len(img.shape) == 3:
                            mid_slice = img.shape[2] // 2
                            slice_img = img[:, :, mid_slice]
                        else:
                            slice_img = img
                        
                        # æ£€æŸ¥å›¾åƒæ•°æ®æœ‰æ•ˆæ€§
                        if slice_img is None or slice_img.size == 0:
                            self.log(f"    è­¦å‘Š: å›¾åƒæ•°æ®æ— æ•ˆï¼Œè·³è¿‡ {result_dir.name}")
                            failed_count += 1
                            continue
                        
                        # ç”Ÿæˆå¯è§†åŒ–
                        success = create_visualization_for_scan(
                            result_data,
                            slice_img,
                            str(result_dir),
                            "visualization.png"
                        )
                        
                        if success:
                            success_count += 1
                            if self.verbose:
                                self.log(f"    âœ“ ç”Ÿæˆå¯è§†åŒ–: {result_dir.name}")
                        else:
                            failed_count += 1
                            if self.verbose:
                                self.log(f"    âœ— ç”Ÿæˆå¤±è´¥: {result_dir.name}")
                                
                    except Exception as e:
                        failed_count += 1
                        if self.verbose:
                            self.log(f"    âœ— å¼‚å¸¸å¤±è´¥ {result_dir.name}: {str(e)[:50]}...")
                
                # 5. æ˜¾ç¤ºå•æ‰«æå¯è§†åŒ–ç”Ÿæˆç»“æœ
                self.log(f"\n   å•æ‰«æå¯è§†åŒ–ç”Ÿæˆå®Œæˆ:")
                self.log(f"     â€¢ æˆåŠŸç”Ÿæˆ: {success_count}")
                self.log(f"     â€¢ ç”Ÿæˆå¤±è´¥: {failed_count}")
                self.log(f"     â€¢ å·²å­˜åœ¨: {len(already_have_viz)}")
            
            # 6. ç”Ÿæˆæ‰¹é‡å¯è§†åŒ–æŠ¥å‘Š
            self.log(f"\nğŸ“Š å¼€å§‹æ‰¹é‡å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆ...")
            
            # ç›´æ¥æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆç»•è¿‡ modules_available æ£€æŸ¥ï¼‰
            import sys
            from pathlib import Path
            batch_viz_path = Path(__file__).parent / "batch_visualization.py"
            
            if not batch_viz_path.exists():
                self.log(f"   âš  æ–‡ä»¶ä¸å­˜åœ¨: {batch_viz_path}")
                self.log("   è¯·ç¡®ä¿ batch_visualization.py åœ¨é¡¹ç›®æ ¹ç›®å½•")
                return True
            
            self.log(f"   âœ“ æ‰¾åˆ°æ–‡ä»¶: {batch_viz_path}")
            
            try:
                # ç¡®ä¿é¡¹ç›®ç›®å½•åœ¨ Python è·¯å¾„ä¸­
                if str(Path(__file__).parent) not in sys.path:
                    sys.path.insert(0, str(Path(__file__).parent))
                
                from batch_visualization import visualize_batch_results
                self.log("   âœ“ æ‰¹é‡å¯è§†åŒ–æ¨¡å—å¯¼å…¥æˆåŠŸ")
                
                # æŸ¥æ‰¾æœ€æ–°çš„æŠ¥å‘Šç›®å½•
                report_dirs = list(self.results_dir.glob("batch_report_*"))
                if not report_dirs:
                    self.log("   âš  æœªæ‰¾åˆ°æŠ¥å‘Šç›®å½•ï¼Œè·³è¿‡æ‰¹é‡å¯è§†åŒ–")
                    return True
                
                latest_report_dir = max(report_dirs, key=lambda x: x.stat().st_mtime)
                self.log(f"   ä½¿ç”¨æœ€æ–°æŠ¥å‘Šç›®å½•: {latest_report_dir.name}")
                
                data_dir = latest_report_dir / "01_detailed_data"
                csv_files = list(data_dir.glob("detailed_results_*.csv"))
                
                if not csv_files:
                    self.log(f"   âš  æœªæ‰¾åˆ°CSVæ–‡ä»¶: {data_dir}")
                    return True
                
                latest_csv = max(csv_files, key=lambda x: x.stat().st_mtime)
                self.log(f"   ä½¿ç”¨æ•°æ®æ–‡ä»¶: {latest_csv.name}")
                
                # åˆ›å»ºæ‰¹é‡å¯è§†åŒ–ç›®å½•
                viz_dir = latest_report_dir / "02_visualizations"
                viz_dir.mkdir(exist_ok=True)
                
                # ç”Ÿæˆæ‰¹é‡å¯è§†åŒ–
                self.log(f"   æ­£åœ¨ç”Ÿæˆæ‰¹é‡å¯è§†åŒ–æŠ¥å‘Š...")
                viz_results = visualize_batch_results(
                    str(latest_csv),
                    str(viz_dir)
                )
                
                if viz_results:
                    if isinstance(viz_results, dict):
                        viz_success = sum(viz_results.values())
                        viz_total = len(viz_results)
                        self.log(f"   âœ“ æ‰¹é‡å¯è§†åŒ–å®Œæˆ: {viz_success}/{viz_total} ä¸ªå›¾è¡¨ç”ŸæˆæˆåŠŸ")
                    else:
                        self.log(f"   âœ“ æ‰¹é‡å¯è§†åŒ–å®Œæˆ")
                
                self.log("âœ… æ‰¹é‡å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
                return True
                
            except ImportError as e:
                self.log(f"âš ï¸  æ— æ³•å¯¼å…¥æ‰¹é‡å¯è§†åŒ–æ¨¡å—: {e}")
                return True
            except Exception as e:
                self.log(f"âŒ æ‰¹é‡å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False
                
        except ImportError as e:
            self.log(f"âš ï¸  æ— æ³•å¯¼å…¥å¯è§†åŒ–æ¨¡å—: {e}")
            self.log("   è·³è¿‡å¯è§†åŒ–ç”Ÿæˆï¼Œç»§ç»­å…¶ä»–æ­¥éª¤")
            return True
        except Exception as e:
            self.log(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if self.verbose:
            self.log("\nğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒæ—¶å­˜åœ¨ä¸¤ä¸ªå¯è§†åŒ–æ–‡ä»¶çš„ç›®å½•
            duplicate_count = 0
            for result_dir in self.results_dir.rglob("**/qa_report.json"):
                result_dir = result_dir.parent
                
                viz1 = result_dir / "visualization.png"
                viz2 = result_dir / "quality_report.png"
                
                if viz1.exists() and viz2.exists():
                    # åˆ é™¤ quality_report.pngï¼Œä¿ç•™ visualization.png
                    viz2.unlink()
                    duplicate_count += 1
            
            if duplicate_count > 0:
                self.log(f"   æ¸…ç† {duplicate_count} ä¸ªé‡å¤çš„å¯è§†åŒ–æ–‡ä»¶")
            
        except Exception as e:
            if self.verbose:
                self.log(f"   æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def _create_latest_symlink(self):
        """åˆ›å»ºæŒ‡å‘æœ€æ–°æŠ¥å‘Šçš„ç¬¦å·é“¾æ¥"""
        try:
            latest_link = self.results_dir / "latest_report"
            
            # ç§»é™¤æ—§çš„ç¬¦å·é“¾æ¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if latest_link.exists():
                if latest_link.is_symlink():
                    latest_link.unlink()
                else:
                    # å¦‚æœæ˜¯ç›®å½•ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                    shutil.rmtree(latest_link)
            
            # åˆ›å»ºæ–°çš„ç¬¦å·é“¾æ¥
            target_dir = f"batch_report_{self.report_timestamp}"
            latest_link.symlink_to(target_dir)
            
            self.log(f"âœ… åˆ›å»ºæœ€æ–°æŠ¥å‘Šç¬¦å·é“¾æ¥: {latest_link} -> {target_dir}")
            
        except Exception as e:
            self.log(f"âš ï¸ åˆ›å»ºç¬¦å·é“¾æ¥å¤±è´¥: {e}")
    
    def _print_directory_tree(self, path: Path, file, prefix: str = "", max_depth: int = 3, depth: int = 0):
        """æ‰“å°ç›®å½•æ ‘"""
        if depth > max_depth:
            file.write(f"{prefix}...\n")
            return
        
        # è·å–ç›®å½•å†…å®¹
        try:
            items = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))
        except:
            return
        
        for i, item in enumerate(items):
            is_last = i == len(items) - 1
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            
            if item.is_dir():
                file.write(f"{prefix}{connector}{item.name}/\n")
                next_prefix = prefix + ("    " if is_last else "â”‚   ")
                self._print_directory_tree(item, file, next_prefix, max_depth, depth + 1)
            else:
                file.write(f"{prefix}{connector}{item.name}\n")
    
    def _print_success_summary(self, duration: float, scan_count: int):
        """æ‰“å°æˆåŠŸæ‘˜è¦"""
        self.log("\n" + "=" * 70)
        self.log("ğŸ‰ è‡ªåŠ¨åŒ–æ‰¹é‡åˆ†æå®Œæˆï¼")
        self.log("=" * 70)
        
        self.log(f"ğŸ“Š åˆ†æç»Ÿè®¡:")
        self.log(f"   â€¢ å¤„ç†æ‰«æ: {scan_count}")
        self.log(f"   â€¢ æ€»è€—æ—¶: {duration:.1f}ç§’")
        if scan_count > 0:
            self.log(f"   â€¢ å¹³å‡æ¯æ‰«æ: {duration/scan_count:.1f}ç§’")
        
        self.log(f"\nğŸ“ ç”ŸæˆæŠ¥å‘Š:")
        self.log(f"   â€¢ æŠ¥å‘ŠID: {self.report_id}")
        self.log(f"   â€¢ æ—¶é—´æˆ³: {self.report_timestamp}")
        self.log(f"   â€¢ æŠ¥å‘Šç›®å½•: batch_report_{self.report_timestamp}/")
        self.log(f"        - 00_executive_summary/ (æ‰§è¡Œæ‘˜è¦)")
        self.log(f"        - 01_detailed_data/ (è¯¦ç»†æ•°æ®)")
        self.log(f"        - 03_quality_analysis/ (è´¨é‡åˆ†æ)")
        self.log(f"        - 04_technical_appendix/ (æŠ€æœ¯é™„å½•)")
        
        # åˆ›å»ºç¬¦å·é“¾æ¥è·¯å¾„
        latest_link = self.results_dir / "latest_report"
        self.log(f"   â€¢ æœ€æ–°æŠ¥å‘Šé“¾æ¥: {latest_link}")
        
        self.log(f"\nğŸš€ ä½¿ç”¨æŒ‡å—:")
        self.log(f"   1. æŸ¥çœ‹æ‘˜è¦: latest_report/00_executive_summary/")
        self.log(f"   2. åˆ†ææ•°æ®: latest_report/01_detailed_data/")
        self.log(f"   3. æŸ¥çœ‹å‚æ•°: latest_report/01_detailed_data/scan_parameters_summary_*.csv")
        self.log(f"   4. æ’æŸ¥é—®é¢˜: latest_report/03_quality_analysis/")
        self.log(f"   5. æŸ¥çœ‹ç´¢å¼•: latest_report/REPORT_INDEX_*.md")
        
        self.log(f"\nğŸ“§ æŠ¥å‘Šä½ç½®: {self.results_dir.absolute()}")
        self.log("=" * 70)
    
    def log(self, message: str):
        """æ—¥å¿—è®°å½•"""
        if self.verbose:
            print(message)
        
        # åŒæ—¶å†™å…¥æ—¥å¿—æ–‡ä»¶
        log_dir = self.paths['logs']
        log_dir.mkdir(exist_ok=True)
        log_file = log_dir / f"autoqa_{self.report_timestamp}.log"
        
        timestamp = datetime.now().strftime("%H:%M:%S")
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"[{timestamp}] {message}\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='MRI AutoQA Launcher - One-command automated batch quality analysis',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # ä½¿ç”¨é»˜è®¤ç›®å½•è¿è¡Œå®Œæ•´åˆ†æ
  python autoqa_launcher.py
  
  # æŒ‡å®šè¾“å…¥è¾“å‡ºç›®å½•
  python autoqa_launcher.py --input /path/to/converted_data --output /path/to/results
  
  # è·³è¿‡å¯è§†åŒ–ç”Ÿæˆï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰
  python autoqa_launcher.py --skip-vis
  
  # å¼ºåˆ¶æ¸…ç†æ—§ç»“æœå¹¶é‡æ–°åˆ†æ
  python autoqa_launcher.py --force-clean
  
  # å®‰é™æ¨¡å¼ï¼ˆä»…é”™è¯¯ä¿¡æ¯ï¼‰
  python autoqa_launcher.py --quiet
  
  # ç»„åˆé€‰é¡¹
  python autoqa_launcher.py --input my_data --output my_results --skip-vis --force-clean

Output Directory Structure:
  autoqa_results/
  â”œâ”€â”€ batch_report_YYYYMMDD_HHMMSS/      # å¸¦æ—¶é—´æˆ³çš„æŠ¥å‘Šç›®å½•
  â”‚   â”œâ”€â”€ 00_executive_summary/          # æ‰§è¡Œæ‘˜è¦
  â”‚   â”œâ”€â”€ 01_detailed_data/              # è¯¦ç»†æ•°æ®ï¼ˆåŒ…å«scan_parameters_summaryï¼‰
  â”‚   â”œâ”€â”€ 02_visualizations/             # å¯è§†åŒ–å›¾è¡¨
  â”‚   â”œâ”€â”€ 03_quality_analysis/           # è´¨é‡åˆ†æ
  â”‚   â”œâ”€â”€ 04_technical_appendix/         # æŠ€æœ¯é™„å½•
  â”‚   â””â”€â”€ REPORT_INDEX_*.md              # æŠ¥å‘Šç´¢å¼•
  â”œâ”€â”€ latest_report -> batch_report_...  # ç¬¦å·é“¾æ¥
  â””â”€â”€ patient_*/                         # åŸå§‹åˆ†æç»“æœ
        """
    )
    
    parser.add_argument('--input', '-i', 
                       help='Input directory containing converted NIfTI files (default: converted_data/)')
    parser.add_argument('--output', '-o',
                       help='Output directory for analysis results (default: autoqa_results/)')
    parser.add_argument('--skip-vis', '-s', action='store_true',
                       help='Skip visualization generation for faster processing')
    parser.add_argument('--force-clean', '-f', action='store_true',
                       help='Force clean output directory before analysis')
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='Quiet mode, only show errors and final summary')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯åŠ¨å™¨
    launcher = AutoQALauncher(verbose=not args.quiet)
    
    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    success = launcher.run_full_pipeline(
        input_dir=args.input,
        output_dir=args.output,
        skip_visualization=args.skip_vis,
        force_clean=args.force_clean
    )
    
    # é€€å‡ºä»£ç 
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()